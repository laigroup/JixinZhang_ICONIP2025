\section{Preliminaries}
\textbf{Satisfiability Problems.\\} 
In propositional logic, a Boolean formula consists of Boolean variables and logical operators such as negations (Â¬), conjunctions ($ \land $), and disjunctions ($\lor$). It is typical to represent Boolean formulas in conjunctive normal form (CNF), expressed as a conjunction of clauses, each of which is a disjunction of literals (a variable or its negation). Given a CNF formula, the SAT problem asks whether there exists an assignment to its variables that can satisfy the formula, while the goal of \#SAT is to count the number of all satisfying solutions.\\
\textbf{Iterative Join Graph Propagation.\\}
IJGP (Iterative Join Graph Propagation) is an approximate inference algorithm primarily used to solve marginal probability computation problems in probabilistic graphical models (such as Markov Random Fields and Bayesian Networks). It constructs a Join Graph and iteratively propagates messages on the graph to achieve efficient calculation of complex probability distributions.
Given a probabilistic graphical model, its joint probability distribution can be expressed as a product of factors:
\begin{equation}
P(X)=\left(\frac{1}{Z}\right)\prod\limits_{i=1}\limits^m\phi_i(C_i)
\end{equation}
where \(\phi_i(C_i)\) is a factor defined on a subset of variables \(C_i\subseteq X\), and
\(\textbf{\textit{Z}}\) is the normalization constant (partition function).
A Join Graph is a structure that decomposes the factor graph into multiple clusters, where each cluster contains a set of variables and their associated factors. The Join Graph satisfies the following properties:\\
Coverage: Each factor \(\phi_i\) must be included in at least one cluster.\\
Connectivity: For any two clusters that share a variable, there exists a path connecting them, and all clusters on the path contain the variable.\\
The join graph in this paper was completed using the external tree decomposition tool flow-cutter, and the tree-width was manually controlled.
In the Join Graph, a message is a function sent from a cluster \(C_i\) to another cluster \(C_j\), defined as:
\begin{equation}
m_{i\rightarrow j}(S_{ij})=\sum\limits_{C_i\backslash S_{ij}}\phi_i(C_i)\prod\limits_{k\in ne(i)\backslash j}m_{k\rightarrow i}(S_{ki})
\end{equation}
\(S_{ij}=C_i\bigcap C_j\) is the set of shared variables between clusters \(C_i\) and \(C_j\),  \(ne(i)\) is the set of neightboring cluster of \(C_i\) and \(\sum\limits_{C_i\backslash S_{ij}}\) denotes summation over variables in \(C_i\backslash S_{ij}\).
The core of the IJGP algorithm is to approximate marginal probabilities by iteratively propagating messages. The specific steps are as follows:Initialize all messages \(m_{i\rightarrow j}\) to uniform distributions,For each cluster \(C_i\), compute the message sent to its neighbor \(C_j\), then, Update messages until convergence or the maximum number of iterations is reached, finally, for each variable X, the marginal probability is the product of all related messages in the clusters containing X:
\begin{equation}
P(X)\propto\prod\limits_{X\in C_i}m_{i\rightarrow j}(S_{ij})
\end{equation}
\textbf{Graph Attention Networks.}\\
The Graph Attention Mechanism (GAT) is a graph neural network model based on attention mechanisms. It dynamically aggregates information from neighboring nodes by computing attention weights between nodes.
For a node v and its neighboe \(u \in N(v)\), the attention weight \(\alpha_{vu}\) is defined as:
\begin{equation}
\alpha_{vu}=\frac{exp(LeakyReLU(a^T[Wh_v\mid\mid Wh_u]))}{\sum\limits_{k\in N(v)}exp(LeakyReLU(a^T[Wh_v\mid\mid Wh_k]))}
\end{equation}
where \(W\in \mathbb{R}^{d'\times d}\) a learnable weight matrix;\(a\in \mathbb{R}^{2d'}\) is a learnable attention vector; \(\mid\mid\) denotes vector concatenation; 
The updated representation \(h'_v\) of node v is obtained by weighted aggregation of information from neighboring nodes:
\begin{equation}
h'_v=\sigma(\sum\limits_{u\in N(v)}\alpha_{vu} Wh_u)
\end{equation}
