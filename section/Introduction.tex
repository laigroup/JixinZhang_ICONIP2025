The propositional model counting problem \#SAT, which is the problem of calculating a satisfactory 
assigned quantity for a given Boolean formula, has applications including software verification
\cite{DBLP:journals/fmsd/ClarkeBRZ01}\cite{DBLP:journals/tcs/IvancicYGGA08},hardware design and 
planning\cite{DBLP:conf/dac/SilvaS00}\cite{DBLP:conf/fmcad/SheeranSS00}\\ \cite{DBLP:conf/aips/DomshlakH06}, 
and is a fundamental challenge in computer science. However, \#SAT is \#P-complete and is therefore 
at least as computationally difficult as the NP-complete problem. Traditional exact \#SAT solvers 
(e.g., DPLL-based)\cite{DBLP:journals/tfs/Guller18}\cite{DBLP:journals/jacm/NieuwenhuisOT06} struggle 
with large-scale CNF formulas due to exponential complexity. While approximate methods 
(e.g., MiniCount-based)\cite{DBLP:journals/anor/KrocSS11} scale better, they often fail to capture 
complex clause-variable dependencies,leading to significant errors in structured instances, so many 
approximate solvers (i.e. methods that provide an estimate of the number of solutions) have been 
developed, which can be scaled up to larger problem sizes,but the approximate methods sacrifice a 
certain degree of accuracy.\\

With the superior learning capabilities of neural networks, various machine learning and deep learning 
methods have recently been proposed for propositional model countingproblems\cite{DBLP:conf/aaai/VaezipoorLWMGSB21}
\cite{Atkari_2019_10}\cite{DBLP:conf/ijcnn/OzolinsFDGZK22}. A series of studies aimed at constructing 
independent neural solvers to solve combinatorial and logical reasoning tasks that directly predict
the number of satisfactory assignments for a given instance\cite{DBLP:conf/iclr/AmizadehMW19}
\cite{DBLP:journals/corr/abs-1903-01969}. Other methods integrate neural modules into classical solvers, 
improving the branch heuristic algorithm in the \#SAT solver\cite{DBLP:conf/aaai/VaezipoorLWMGSB21}.
Although neural network methods show great potential to solve \#SAT, they still face some challenges. 
Existing GNN models (such as NSNet\cite{DBLP:conf/nips/LiS22}, BPNN\cite{DBLP:conf/nips/KuckCTLSSE20}) 
rely on simple belief propagation (BP) and cannot effectively model higher-order dependencies and global 
constraint structures between variables. Furthermore, how to combine the deep learning model with the 
strict reasoning of symbolic logic while ensuring that the deep learning model has good expressiveness?
\cite{DBLP:journals/tkde/WuLL24} How to enhance the reasoning ability of the model for complex 
constraints?\cite{DBLP:journals/pami/ChenJYZ22}\\

In order to solve these problems, we propose the AEIN model in this paper. In this work, AEIN bridges 
the neural-symbolic gap by integrating the Iterative Join Graph Propagation  algorithm (IJGP)
\cite{DBLP:journals/corr/abs-1301-0564} into GNNs.   Specifically, we use GAT layers to simulate IJGP's 
belief updates, while tree decomposition guides hierarchical attention (intra-cluster and inter-cluster 
message passing). Inspired by HAN(Zichao Yang et al.,2016)\cite{DBLP:conf/naacl/YangYDHSH16}, this 
hierarchical attention method is a text classification model in which attention mechanisms are applied in 
layers of text. At the same time, in order to reduce the extra verhead brought by the attention mechanism, 
we adopt the method of dynamic pruning attention heads with the timestep\cite{DBLP:conf/icip/LiCLCSQW23}
\cite{DBLP:journals/nn/ZhangLWWW24}, which has more attention heads at the early stage of iteration and 
reduces them successively to avoid redundant calculations.As a focus of this paper, the constraint-aware 
attention mechanism encourages variable values to tend to satisfy clauses by showing modeling constraints 
(calculating attention scores) as regularization terms in the loss function, guiding the first two kinds 
of attention simultaneously. \\

The partition function has been shown to be a good approximation of the number of Boolean formula 
solutions\cite{Bethe_1997_07}\cite{DBLP:journals/siamdm/ChandrasekaranCGSS11}, and our model similarly 
follows this computational approach, with careful and extensive experimental investigation, and experiments 
on BIRD and SATLIB benchmarks show that our method is significantly superior to NSNet. 
